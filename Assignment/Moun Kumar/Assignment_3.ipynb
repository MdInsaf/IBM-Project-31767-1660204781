{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66881efa",
      "metadata": {
        "id": "66881efa"
      },
      "source": [
        "# DOWNLOAD AND LOAD THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99ca4ba",
      "metadata": {
        "id": "f99ca4ba"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3008d67f",
      "metadata": {
        "id": "3008d67f"
      },
      "source": [
        "# IMAGE AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fa1159",
      "metadata": {
        "id": "16fa1159"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f8e46b",
      "metadata": {
        "id": "28f8e46b"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9592f369",
      "metadata": {
        "id": "9592f369",
        "outputId": "5ad04cbe-4d2e-49d5-9cdc-14f0917a0b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4317 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "xtrain = train_datagen.flow_from_directory('flowers',\n",
        "                                           target_size=(64,64),\n",
        "                                           class_mode='categorical',\n",
        "                                           batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d3f2a6",
      "metadata": {
        "id": "58d3f2a6",
        "outputId": "8dd19d33-3d4a-4377-a3f8-d6f4ba7534e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4317 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "xtest = train_datagen.flow_from_directory('flowers',\n",
        "                                           target_size=(64,64),\n",
        "                                           class_mode='categorical',\n",
        "                                           batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad06119",
      "metadata": {
        "id": "8ad06119"
      },
      "source": [
        "# CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f910dbc",
      "metadata": {
        "id": "4f910dbc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6064c899",
      "metadata": {
        "id": "6064c899"
      },
      "source": [
        "# ADDING LAYERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491d142d",
      "metadata": {
        "id": "491d142d"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(300,activation='relu'))\n",
        "model.add(Dense(150,activation='relu'))\n",
        "model.add(Dense(5,activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f141da",
      "metadata": {
        "id": "57f141da"
      },
      "source": [
        "# COMPILING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfee3dd0",
      "metadata": {
        "id": "cfee3dd0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b15c74",
      "metadata": {
        "id": "f5b15c74"
      },
      "source": [
        "# FITTING THE MODEL AND TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623edcf8",
      "metadata": {
        "id": "623edcf8"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeddab7e",
      "metadata": {
        "id": "eeddab7e"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
        "                        patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                        patience=5,\n",
        "                        factor=0.5,min_lr=0.00001)\n",
        "\n",
        "callback = [reduce_lr,early_stopping]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595c941e",
      "metadata": {
        "id": "595c941e",
        "outputId": "6681383b-530d-40fa-e2f3-1e7722f12fdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_4264/2338346036.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(xtrain,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 454s 10s/step - loss: 1.5832 - accuracy: 0.3841 - val_loss: 1.1629 - val_accuracy: 0.5240\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 162s 4s/step - loss: 1.1591 - accuracy: 0.5237 - val_loss: 1.1496 - val_accuracy: 0.5332\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 155s 4s/step - loss: 1.0605 - accuracy: 0.5784 - val_loss: 0.9839 - val_accuracy: 0.6155\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 161s 4s/step - loss: 0.9860 - accuracy: 0.6134 - val_loss: 0.9281 - val_accuracy: 0.6324\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 163s 4s/step - loss: 0.9186 - accuracy: 0.6498 - val_loss: 0.8870 - val_accuracy: 0.6551\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 150s 3s/step - loss: 0.8737 - accuracy: 0.6625 - val_loss: 0.8345 - val_accuracy: 0.6745\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 158s 4s/step - loss: 0.8350 - accuracy: 0.6866 - val_loss: 0.8257 - val_accuracy: 0.6778\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 140s 3s/step - loss: 0.8006 - accuracy: 0.6838 - val_loss: 0.7570 - val_accuracy: 0.7116\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 152s 3s/step - loss: 0.7634 - accuracy: 0.7091 - val_loss: 0.7210 - val_accuracy: 0.7306\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 158s 4s/step - loss: 0.7187 - accuracy: 0.7197 - val_loss: 0.6681 - val_accuracy: 0.7470\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x251b6dd0a30>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit_generator(xtrain,\n",
        "                    steps_per_epoch=len(xtrain),\n",
        "                    epochs=10,\n",
        "                    validation_data=xtest,\n",
        "                    validation_steps=len(xtest))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad229d3",
      "metadata": {
        "id": "bad229d3"
      },
      "source": [
        "# SAVING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce99a35e",
      "metadata": {
        "id": "ce99a35e"
      },
      "outputs": [],
      "source": [
        "model.save('Flower.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce0830e",
      "metadata": {
        "id": "cce0830e"
      },
      "source": [
        "# TESTING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d15522",
      "metadata": {
        "id": "f6d15522"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c866400e",
      "metadata": {
        "id": "c866400e"
      },
      "outputs": [],
      "source": [
        "img = image.load_img('flowers/daisy/14163875973_467224aaf5_m.jpg',target_size=(64,64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414a921b",
      "metadata": {
        "id": "414a921b",
        "outputId": "7023fffc-e897-4d78-8727-db40b0bec27e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[ 35.,  36.,  30.],\n",
              "        [ 33.,  34.,  28.],\n",
              "        [ 24.,  25.,  19.],\n",
              "        ...,\n",
              "        [ 17.,  20.,   0.],\n",
              "        [ 11.,  18.,   0.],\n",
              "        [ 12.,  21.,   0.]],\n",
              "\n",
              "       [[ 43.,  44.,  38.],\n",
              "        [ 41.,  42.,  36.],\n",
              "        [ 26.,  27.,  21.],\n",
              "        ...,\n",
              "        [ 16.,  25.,   6.],\n",
              "        [ 12.,  28.,   0.],\n",
              "        [ 16.,  33.,   0.]],\n",
              "\n",
              "       [[ 48.,  49.,  41.],\n",
              "        [ 43.,  44.,  36.],\n",
              "        [ 28.,  29.,  21.],\n",
              "        ...,\n",
              "        [ 24.,  36.,  14.],\n",
              "        [ 27.,  46.,   0.],\n",
              "        [ 30.,  49.,   3.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 33.,  43.,   9.],\n",
              "        [ 49.,  67.,  15.],\n",
              "        [ 44.,  64.,   5.],\n",
              "        ...,\n",
              "        [136., 135., 140.],\n",
              "        [136., 135., 140.],\n",
              "        [146., 145., 150.]],\n",
              "\n",
              "       [[ 71.,  54.,  44.],\n",
              "        [ 61.,  61.,  51.],\n",
              "        [ 45.,  68.,   0.],\n",
              "        ...,\n",
              "        [143., 142., 140.],\n",
              "        [148., 147., 145.],\n",
              "        [158., 158., 156.]],\n",
              "\n",
              "       [[ 32.,  21.,   3.],\n",
              "        [106.,   5.,  35.],\n",
              "        [ 62.,  48.,   9.],\n",
              "        ...,\n",
              "        [132., 131., 126.],\n",
              "        [146., 145., 143.],\n",
              "        [153., 153., 153.]]], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = image.img_to_array(img)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e15d4a",
      "metadata": {
        "id": "24e15d4a",
        "outputId": "811ca88a-3622-4277-a614-79284256b395"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[ 35.,  36.,  30.],\n",
              "         [ 33.,  34.,  28.],\n",
              "         [ 24.,  25.,  19.],\n",
              "         ...,\n",
              "         [ 17.,  20.,   0.],\n",
              "         [ 11.,  18.,   0.],\n",
              "         [ 12.,  21.,   0.]],\n",
              "\n",
              "        [[ 43.,  44.,  38.],\n",
              "         [ 41.,  42.,  36.],\n",
              "         [ 26.,  27.,  21.],\n",
              "         ...,\n",
              "         [ 16.,  25.,   6.],\n",
              "         [ 12.,  28.,   0.],\n",
              "         [ 16.,  33.,   0.]],\n",
              "\n",
              "        [[ 48.,  49.,  41.],\n",
              "         [ 43.,  44.,  36.],\n",
              "         [ 28.,  29.,  21.],\n",
              "         ...,\n",
              "         [ 24.,  36.,  14.],\n",
              "         [ 27.,  46.,   0.],\n",
              "         [ 30.,  49.,   3.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 33.,  43.,   9.],\n",
              "         [ 49.,  67.,  15.],\n",
              "         [ 44.,  64.,   5.],\n",
              "         ...,\n",
              "         [136., 135., 140.],\n",
              "         [136., 135., 140.],\n",
              "         [146., 145., 150.]],\n",
              "\n",
              "        [[ 71.,  54.,  44.],\n",
              "         [ 61.,  61.,  51.],\n",
              "         [ 45.,  68.,   0.],\n",
              "         ...,\n",
              "         [143., 142., 140.],\n",
              "         [148., 147., 145.],\n",
              "         [158., 158., 156.]],\n",
              "\n",
              "        [[ 32.,  21.,   3.],\n",
              "         [106.,   5.,  35.],\n",
              "         [ 62.,  48.,   9.],\n",
              "         ...,\n",
              "         [132., 131., 126.],\n",
              "         [146., 145., 143.],\n",
              "         [153., 153., 153.]]]], dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.expand_dims(x,axis=0)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6357fd7",
      "metadata": {
        "id": "d6357fd7",
        "outputId": "d3314d62-894d-4337-ae1a-210d50d47ac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4d343d",
      "metadata": {
        "id": "7c4d343d",
        "outputId": "53614f44-78ac-42b5-a909-f7704aba0d29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'daisy'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "op = ['daisy','dandelion','rose','sunflower','tulip']\n",
        "pred = np.argmax(model.predict(x))\n",
        "op[pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e051e0c",
      "metadata": {
        "id": "8e051e0c",
        "outputId": "d157339d-073d-42fd-ea88-1edc77323626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rose'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img = image.load_img('flowers/rose/10894627425_ec76bbc757_n.jpg',target_size=(64,64))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x,axis=0)\n",
        "pred = np.argmax(model.predict(x))\n",
        "op[pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395a857c",
      "metadata": {
        "id": "395a857c",
        "outputId": "a66779c8-c49e-4627-adc3-85eef3e41001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dandelion'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img = image.load_img('flowers/dandelion/16041975_2f6c1596e5.jpg',target_size=(64,64))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x,axis=0)\n",
        "pred = np.argmax(model.predict(x))\n",
        "op[pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b21308",
      "metadata": {
        "id": "a2b21308"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}